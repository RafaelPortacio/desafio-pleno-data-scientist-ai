"""
Agente de IA para an√°lise de dados da Prefeitura do Rio de Janeiro
Utiliza LangGraph para orquestrar um fluxo de an√°lise de dados inteligente
"""

import warnings
import logging

# Suprimir warnings de telemetria do ChromaDB
warnings.filterwarnings("ignore", message=".*capture.*takes.*positional argument.*")
logging.getLogger("chromadb").setLevel(logging.ERROR)

# Suprimir warnings adicionais
warnings.filterwarnings("ignore", message=".*Failed to send telemetry event.*")
import sys
if hasattr(sys, 'stderr'):
    original_stderr = sys.stderr
    
    class SuppressStderr:
        def write(self, data):
            # Suprimir apenas mensagens de telemetria
            if 'Failed to send telemetry event' not in data and 'capture() takes' not in data:
                original_stderr.write(data)
        def flush(self):
            original_stderr.flush()
    
    # Redirecionar stderr temporariamente
    import contextlib
    @contextlib.contextmanager
    def suppress_chromadb_warnings():
        old_stderr = sys.stderr
        sys.stderr = SuppressStderr()
        try:
            yield
        finally:
            sys.stderr = old_stderr

from typing import Dict, Any, List, Optional, TypedDict, Literal
import pandas as pd
import os
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage
from google.cloud import bigquery
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.tools import tool
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

from langgraph.graph import StateGraph, END
from langgraph.checkpoint.memory import MemorySaver

# Importar tools de categoria
from category_tools import CATEGORY_TOOLS

# Load environment variables
load_dotenv()

class AgentState(TypedDict):
    """Estado compartilhado entre os n√≥s do agente"""
    question: str
    intent: Optional[str]
    sql_query: Optional[str] 
    data_result: Optional[list]
    final_response: str
    error: Optional[str]
    messages: List[Dict[str, str]]

class DataAnalystAgent:
    """
    Agente analista de dados usando LangGraph para orquestrar consultas ao BigQuery
    """
    
    def __init__(self):
        """
        Inicializa o agente
        """
        self.llm = self._setup_llm()
        self.bigquery_client = self._setup_bigquery()
        self._ensure_chromadb_initialized()
        self.graph = self._build_graph()
        
    def _ensure_chromadb_initialized(self):
        """Verifica se os ChromaDBs existem e os inicializa se necess√°rio"""
        import chromadb
        
        try:
            # Conectar ao ChromaDB com supress√£o de warnings
            with suppress_chromadb_warnings():
                client = chromadb.PersistentClient(path="./chroma_db")
                
                # Verificar se todas as cole√ß√µes necess√°rias existem
                required_collections = ['tipo', 'subtipo', 'nome_unidade_organizacional', 'id_unidade_organizacional_mae']
                existing_collections = [col.name for col in client.list_collections()]
                
                missing_collections = [col for col in required_collections if col not in existing_collections]
                
                if missing_collections:
                    print(f"üîÑ Cole√ß√µes ausentes: {missing_collections}. Inicializando embeddings...")
                    self._initialize_embeddings()
                else:
                    print("‚úÖ Todas as cole√ß√µes ChromaDB encontradas.")
                    
        except Exception as e:
            print(f"üîÑ Erro ao verificar ChromaDB: {e}. Inicializando embeddings...")
            self._initialize_embeddings()
    
    def _initialize_embeddings(self):
        """Inicializa os embeddings das colunas categ√≥ricas"""
        try:
            from initialize_embeddings import CategoryEmbeddingsInitializer
            initializer = CategoryEmbeddingsInitializer()
            
            print("üìä Inicializando todas as cole√ß√µes de embeddings...")
            initializer.initialize_all_collections()
            
            print("üéâ Embeddings inicializados com sucesso!")
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao inicializar embeddings: {e}")
            print("   O agente continuar√° funcionando sem as tools de similaridade.")
        
    def _setup_llm(self):
        """Configura o modelo de linguagem OpenAI"""
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            raise ValueError("OPENAI_API_KEY n√£o encontrada no arquivo .env")
        
        return ChatOpenAI(
            model="gpt-5",
            temperature=1,
            api_key=api_key
        )
    
    def _setup_bigquery(self):
        """Configura cliente BigQuery usando chave de servi√ßo JSON"""
        key_path = os.getenv("GOOGLE_APPLICATION_CREDENTIALS")
        
        if not key_path:
            raise Exception("GOOGLE_APPLICATION_CREDENTIALS n√£o encontrada no .env")
        
        if not os.path.exists(key_path):
            raise Exception(f"Arquivo de chave n√£o encontrado: {key_path}")
        
        try:
            print(f"üîë Conectando ao BigQuery usando: {key_path}")
            return bigquery.Client.from_service_account_json(key_path)
        except Exception as e:
            raise Exception(f"Erro ao configurar BigQuery: {e}")
    
    def _build_graph(self) -> StateGraph:
        """Constr√≥i o grafo LangGraph"""
        # Criar o grafo
        workflow = StateGraph(AgentState)
        
        # Adicionar n√≥s
        workflow.add_node("router", self.router_node)
        workflow.add_node("sql_generator", self.sql_generator_node)
        workflow.add_node("sql_executor", self.sql_executor_node)
        workflow.add_node("response_synthesizer", self.response_synthesizer_node)
        workflow.add_node("conversational_responder", self.conversational_responder_node)
        
        # Configurar ponto de entrada
        workflow.set_entry_point("router")
        
        # Adicionar roteamento condicional
        workflow.add_conditional_edges(
            "router",
            self.route_intent,
            {
                "data_query": "sql_generator",
                "conversational": "conversational_responder"
            }
        )
        
        # Fluxo linear para consultas de dados
        workflow.add_edge("sql_generator", "sql_executor")
        workflow.add_edge("sql_executor", "response_synthesizer")
        workflow.add_edge("response_synthesizer", END)
        workflow.add_edge("conversational_responder", END)
        
        # Compilar com checkpoint
        memory = MemorySaver()
        return workflow.compile(checkpointer=memory)
    
    def router_node(self, state: AgentState) -> AgentState:
        """
        N√≥ roteador: determina se a pergunta requer dados ou √© conversacional
        """
        question = state["question"]
        
        prompt = f"""
        Analise a seguinte pergunta e determine se ela requer consulta a dados ou √© conversacional.

        Pergunta: "{question}"

        Responda APENAS com uma das op√ß√µes:
        - "data_query" se a pergunta for sobre dados/estat√≠sticas (ex: quantos, qual, quais, como, quando sobre chamados, bairros, etc.)
        - "conversational" se for sauda√ß√£o, agradecimento, pergunta gen√©rica ou n√£o relacionada a dados espec√≠ficos

        Exemplos:
        - "Quantos chamados foram abertos?" -> data_query
        - "Qual o bairro com mais chamados?" -> data_query
        - "Ol√°, tudo bem?" -> conversational
        - "Obrigado!" -> conversational
        - "Me d√™ sugest√µes de brincadeiras" -> conversational
        """
        
        response = self.llm.invoke([HumanMessage(content=prompt)])
        intent = response.content.strip().lower()
        
        # Validar resposta
        if intent not in ["data_query", "conversational"]:
            intent = "conversational"  # Default para conversacional em caso de d√∫vida
        
        state["intent"] = intent
        state["messages"].append({"role": "system", "content": f"Intent classificado como: {intent}"})
        
        return state
    
    def route_intent(self, state: AgentState) -> Literal["data_query", "conversational"]:
        """Fun√ß√£o de roteamento para o conditional_edges"""
        return state["intent"]
    
    def sql_generator_node(self, state: AgentState) -> AgentState:
        """
        N√≥ gerador de SQL: agente com tools para encontrar valores categ√≥ricos similares
        """
        question = state["question"]
        
        with open("schema_chamado.txt", "r", encoding="utf-8") as f:
            schema_chamado = f.read()
            
        with open("schema_bairro.txt", "r", encoding="utf-8") as f:
            schema_bairro = f.read()

        # Criar prompt para o agente com tools
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", """Voc√™ √© um especialista em SQL para BigQuery. 

        INSTRU√á√ïES PARA USO DAS TOOLS:
        - Se a pergunta mencionar termos que podem corresponder a colunas categ√≥ricas, use as tools para encontrar valores exatos
        - Tools dispon√≠veis:
        * get_nome_unidade_organizacional: Para buscar unidades organizacionais
        * get_id_unidade_organizacional_mae: Para buscar unidades m√£e  
        * get_tipo: Para buscar tipos de chamados
        * get_subtipo: Para buscar subtipos espec√≠ficos
        - Use as tools ANTES de gerar o SQL para garantir valores corretos

        Primeiro, fa√ßa um REASONING sobre a descri√ß√£o do schema e como ela consegue atender √† pergunta.

        Depois gere uma consulta SQL otimizada.

        INFORMA√á√ïES IMPORTANTES:
        - Tabela principal: `datario.adm_central_atendimento_1746.chamado`
        - Tabela de bairros: `datario.dados_mestres.bairro`
        - Atente-se ao uso da coluna correta de data em suas consultas
        - Evite SELECT * - selecione apenas colunas necess√°rias
        - Use LIMIT quando apropriado para evitar resultados excessivos

        DESCRI√á√ÉO DA TABELA DE CHAMADOS:
        {schema_chamado}

        DESCRI√á√ÉO DA TABELA DADOS DOS BAIRROS:
        {schema_bairro}

        PARA JOINS COM BAIRROS:
        - Use: JOIN `datario.dados_mestres.bairro`

        INSTRU√á√ïES:
        1. Use agrega√ß√µes (COUNT, GROUP BY) quando apropriado
        2. Para top N, use ORDER BY e LIMIT
        3. Se mencionar nomes de bairros, fa√ßa JOIN com a tabela de bairros
        4. O SQL gerado ser√° executado imediatamente, n√£o adicione explica√ß√µes ou coment√°rios
        5. O subtipo √© subordinado ao tipo, prefira usar o tipo para filtrar primeiro e se n√£o for o suficiente, use o subtipo para refinar a busca
        6. Evite usar queries com LIKE, elas s√£o extremamente, as tools s√£o para encontrar o valor exato correto (Por exemplo, se a pergunta for encontre subtipos relacionados a "Regras de Tr√¢nsito" use a tool get_tipo para encontrar o tipo exato para filtrar pelo nome dele na query, caso n√£o haja NENHUM adequado, a√≠ procure por subtipos relacionados e filtre por eles)
        
        {agent_scratchpad}

        Formato:
        REASONING: [seu racioc√≠nio]
        SQL: [apenas o c√≥digo SQL]"""),
            ("human", "Pergunta: {question}")
        ])

        # Bind tools to LLM
        llm_with_tools = self.llm.bind_tools(CATEGORY_TOOLS)
        
        # Create chain
        chain = prompt_template | llm_with_tools
        
        # Execute with tools
        print("ü§ñ Executando LLM com tools...")
        result = chain.invoke({
            "question": question,
            "schema_chamado": schema_chamado,
            "schema_bairro": schema_bairro,
            "agent_scratchpad": ""
        })
        
        # Verificar se h√° tool calls
        if hasattr(result, 'tool_calls') and result.tool_calls:
            print("üîß Executando tool calls...")
            
            # Executar cada tool call
            tool_results = []
            for tool_call in result.tool_calls:
                tool_name = tool_call['name']
                tool_args = tool_call['args']
                
                print(f"  üõ†Ô∏è Executando {tool_name} com args: {tool_args}")
                
                # Encontrar e executar a tool
                for tool in CATEGORY_TOOLS:
                    if tool.name == tool_name:
                        try:
                            tool_result = tool.func(**tool_args)
                            tool_results.append(f"{tool_name}: {tool_result}")
                            print(f"  ‚úÖ Resultado: {tool_result}")
                        except Exception as e:
                            error_msg = f"Erro em {tool_name}: {e}"
                            tool_results.append(error_msg)
                            print(f"  ‚ùå {error_msg}")
                        break
            
            # Gerar nova consulta com os resultados das tools
            tool_context = "\n".join(tool_results)
            
            print("üîÑ Gerando SQL com resultados das tools...")
            
            # Novo prompt com contexto das tools
            new_prompt = ChatPromptTemplate.from_messages([
                ("system", """Voc√™ √© um especialista em SQL para BigQuery.

                INSTRU√á√ïES:
                - Use os resultados das tools abaixo para gerar a consulta SQL
                - Se as tools n√£o encontraram resultados similares, use LIKE com wildcards para buscar termos relacionados
                - NUNCA retorne [] ou SQL inv√°lida - sempre gere uma consulta v√°lida
                - Se n√£o encontrou valores exatos, use termos mais gen√©ricos ou padr√µes LIKE
                
                RESULTADOS DAS TOOLS:
                {tool_context}

                DESCRI√á√ÉO DA TABELA DE CHAMADOS:
                {schema_chamado}

                DESCRI√á√ÉO DA TABELA DADOS DOS BAIRROS:
                {schema_bairro}

                INSTRU√á√ïES PARA FALLBACK:
                - Se n√£o encontrou "Ilumina√ß√£o P√∫blica", use LIKE '%ilumina√ß√£o%' ou '%l√¢mpada%' ou '%poste%'
                - Se n√£o encontrou "reparo de buraco", use LIKE '%buraco%' ou '%pavimenta√ß√£o%' ou '%via%'
                - Se n√£o encontrou "fiscaliza√ß√£o estacionamento", use LIKE '%fiscaliza√ß√£o%' e '%estacionamento%'
                - Sempre prefira gerar SQL funcional mesmo que aproximada
                - SEMPRE use nomes completos de tabelas: `datario.adm_central_atendimento_1746.chamado` e `datario.dados_mestres.bairro`
                - NUNCA use apenas "chamado" ou "bairro" - sempre com o dataset completo

                Primeiro, fa√ßa um REASONING sobre como usar os resultados das tools ou fallback.
                Depois gere uma consulta SQL otimizada e V√ÅLIDA.

                Formato:
                REASONING: [seu racioc√≠nio]
                SQL: [apenas o c√≥digo SQL v√°lido]"""),
                ("human", "Pergunta: {question}")
            ])
            
            # Nova chain sem tools
            new_chain = new_prompt | self.llm
            
            # Executar com contexto das tools
            new_result = new_chain.invoke({
                "question": question,
                "tool_context": tool_context,
                "schema_chamado": schema_chamado,
                "schema_bairro": schema_bairro
            })
            
            full_response = new_result.content
            
        else:
            full_response = result.content
        
        # Extrair reasoning e SQL
        if "SQL:" in full_response:
            parts = full_response.split("SQL:")
            reasoning = parts[0].replace("REASONING:", "").strip()
            sql_query = parts[1].strip()
            print(f"üß† REASONING: {reasoning}")
        else:
            reasoning = "Reasoning n√£o fornecido pelo LLM."
            sql_query = full_response
            
        # Validar SQL antes de prosseguir
        if not sql_query or sql_query.strip() == "" or sql_query.strip() == "[]" or "SELECT NULL" in sql_query.upper():
            print("‚ö†Ô∏è SQL inv√°lida ou vazia detectada. Gerando fallback...")
            # Tentar fallback simples baseado na pergunta
            question_lower = state["question"].lower()
            if "ilumina√ß√£o" in question_lower:
                sql_query = """
                SELECT subtipo, COUNT(*) as total
                FROM `datario.adm_central_atendimento_1746.chamado`
                WHERE (LOWER(tipo) LIKE '%ilumina√ß√£o%' OR LOWER(subtipo) LIKE '%l√¢mpada%' OR LOWER(subtipo) LIKE '%poste%')
                GROUP BY subtipo
                ORDER BY total DESC
                LIMIT 5
                """
            elif "buraco" in question_lower:
                sql_query = """
                SELECT b.nome as bairro, COUNT(*) as chamados
                FROM `datario.adm_central_atendimento_1746.chamado` c
                JOIN `datario.dados_mestres.bairro` b ON c.id_bairro = b.id_bairro
                WHERE (LOWER(c.tipo) LIKE '%pavimenta√ß√£o%' OR LOWER(c.subtipo) LIKE '%buraco%' OR LOWER(c.subtipo) LIKE '%via%')
                AND DATE(c.data_inicio) BETWEEN '2023-01-01' AND '2023-12-31'
                GROUP BY b.nome
                ORDER BY chamados DESC
                LIMIT 3
                """
            elif "fiscaliza√ß√£o" in question_lower and "estacionamento" in question_lower:
                sql_query = """
                SELECT nome_unidade_organizacional, COUNT(*) as total
                FROM `datario.adm_central_atendimento_1746.chamado`
                WHERE (LOWER(subtipo) LIKE '%fiscaliza√ß√£o%' AND LOWER(subtipo) LIKE '%estacionamento%')
                GROUP BY nome_unidade_organizacional
                ORDER BY total DESC
                LIMIT 1
                """
            else:
                sql_query = "SELECT 'N√£o foi poss√≠vel gerar consulta para esta pergunta' as mensagem"
            
            print(f"üîÑ Usando SQL de fallback: {sql_query.strip()}")
        
        sql_query = sql_query.strip()
        
        # Limpar marcadores de c√≥digo do SQL
        if sql_query.startswith("```sql"):
            sql_query = sql_query[6:]
        if sql_query.endswith("```"):
            sql_query = sql_query[:-3]
        
        sql_query = sql_query.strip()
        
        # Armazenar no estado
        state["sql_query"] = sql_query
        state["messages"].append({"role": "system", "content": f"SQL gerado: {sql_query}"})
        
        return state
    
    def sql_executor_node(self, state: AgentState) -> AgentState:
        """
        N√≥ executor de SQL: executa a consulta no BigQuery
        """
        sql_query = state["sql_query"]
        
        if not sql_query or sql_query.strip() == "":
            print("‚ùå ERRO: SQL query est√° vazia!")
            state["error"] = "SQL query vazia ou inv√°lida"
            state["data_result"] = []
            return state
        
        # try:
            # Executar consulta
        print(f"üîç Executando SQL:\n{sql_query}")
        results = self.bigquery_client.query(sql_query)
        
        # Converter para DataFrame
        df = results.to_dataframe().to_dict(orient="records")
        print(df)
        state["data_result"] = df
        state["messages"].append({
            "role": "system", 
            "content": f"Consulta executada com sucesso. {len(df)} linhas retornadas."
        })
            
        # except Exception as e:
        #     error_msg = f"Erro ao executar SQL: {str(e)}"
        #     print(error_msg)
        #     state["error"] = error_msg
        #     state["messages"].append({"role": "system", "content": error_msg})
            
        #     # Criar DataFrame vazio em caso de erro
        #     state["data_result"] = []
        
        return state
    
    def response_synthesizer_node(self, state: AgentState) -> AgentState:
        """
        N√≥ sintetizador: converte dados em resposta natural
        """
        question = state["question"]
        data_result = state["data_result"]
        error = state.get("error")
        
        if error:
            # Se houve erro, gerar resposta de erro amig√°vel
            prompt = f"""
            Houve um erro ao processar a consulta de dados para a pergunta: "{question}"
            
            Erro: {error}
            
            Forne√ßa uma resposta amig√°vel explicando que n√£o foi poss√≠vel obter os dados solicitados
            e sugira que o usu√°rio reformule a pergunta ou tente novamente.
            """
        else:
            # Se temos dados, sintetizar resposta
            if len(data_result):
                # Criar resumo dos dados
                data_summary = f"Dados encontrados ({len(data_result)} linhas):\n"
                if len(data_result) <= 10:
                    data_summary += str(data_result)
                else:
                    data_summary += str(data_result[:10])
                    data_summary += f"\n... e mais {len(data_result) - 10} linhas"
            else:
                data_summary = "Nenhum resultado encontrado."
            
            prompt = f"""
            Baseado nos dados consultados, responda √† pergunta de forma clara e objetiva.

            Pergunta: "{question}"

            Dados:
            {data_summary}

            INSTRU√á√ïES:
            1. Responda em portugu√™s brasileiro
            2. Seja espec√≠fico e cite os n√∫meros encontrados
            3. Se houver m√∫ltiplos resultados, destaque os principais
            4. Mantenha a resposta focada na pergunta
            5. Se n√£o houver dados, explique que n√£o foram encontrados resultados para os crit√©rios especificados
            """
        
        response = self.llm.invoke([HumanMessage(content=prompt)])
        final_response = response.content.strip()
        
        state["final_response"] = final_response
        state["messages"].append({"role": "assistant", "content": final_response})
        
        return state
    
    def conversational_responder_node(self, state: AgentState) -> AgentState:
        """
        N√≥ para respostas conversacionais (sauda√ß√µes, agradecimentos, etc.)
        """
        question = state["question"]
        
        prompt = f"""
        Responda de forma amig√°vel e natural √† seguinte mensagem conversacional:
        
        "{question}"
        
        CONTEXTO: Voc√™ √© um assistente de an√°lise de dados da Prefeitura do Rio de Janeiro.
        
        INSTRU√á√ïES:
        1. Seja cordial e profissional
        2. Se for sauda√ß√£o, retribua e se apresente brevemente
        3. Se for agradecimento, responda educadamente
        4. Se for pergunta gen√©rica n√£o relacionada a dados, responda de forma extremamente concisa e redirecione para suas capacidades de an√°lise de dados.
        5. Mantenha respostas concisas
        """
        
        response = self.llm.invoke([HumanMessage(content=prompt)])
        final_response = response.content.strip()
        
        state["final_response"] = final_response
        state["messages"].append({"role": "assistant", "content": final_response})
        
        return state
    
    def run(self, question: str, config: Optional[Dict] = None) -> str:
        """
        Executa o agente com uma pergunta
        
        Args:
            question: Pergunta do usu√°rio
            config: Configura√ß√£o opcional para o grafo
            
        Returns:
            Resposta final do agente
        """
        # Estado inicial
        initial_state = {
            "question": question,
            "intent": None,
            "sql_query": None,
            "data_result": None,
            "final_response": "",
            "error": None,
            "messages": [{"role": "user", "content": question}]
        }
        
        # Configura√ß√£o padr√£o
        if config is None:
            config = {"configurable": {"thread_id": "default"}}
        
        # Executar grafo
        result = self.graph.invoke(initial_state, config=config)
        
        return result["final_response"]

def main():
    """Fun√ß√£o principal para testar o agente"""
    # Perguntas de teste dos requisitos
    test_questions = [
        "Quantos chamados foram abertos no dia 28/11/2024?",
        "Qual o subtipo de chamado mais comum relacionado a 'Ilumina√ß√£o P√∫blica'?",
        "Quais os 3 bairros que mais tiveram chamados abertos sobre 'reparo de buraco' em 2023?",
        "Qual o nome da unidade organizacional que mais atendeu chamados de 'Fiscaliza√ß√£o de estacionamento irregular'?",
        "Ol√°, tudo bem?",
        "Me d√™ sugest√µes de brincadeiras para fazer com meu cachorro!"
    ]
    
    try:
        print("üöÄ Inicializando Agente de An√°lise de Dados...")
        agent = DataAnalystAgent()
        print("‚úÖ Agente inicializado com sucesso!")
        
        # Testar perguntas
        for i, question in enumerate(test_questions, 1):
            print(f"\n{'='*50}")
            print(f"TESTE {i}: {question}")
            print('='*50)
            
            try:
                response = agent.run(question)
                print(f"ü§ñ Resposta: {response}")
            except Exception as e:
                print(f"‚ùå Erro: {e}")
        
    except Exception as e:
        print(f"‚ùå Erro ao inicializar agente: {e}")
        print("\nüîß Verifique se:")
        print("   1. OPENAI_API_KEY est√° configurada no .env")
        print("   2. GOOGLE_APPLICATION_CREDENTIALS aponta para o arquivo JSON correto")
        print("   3. pip install -r requirements.txt foi executado")

if __name__ == "__main__":
    main()